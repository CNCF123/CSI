Kubernetes 版本：
$ kubectl get node
NAME          STATUS   ROLES    AGE   VERSION
k8s-master-1  Ready    master   23d   v1.18.1
k8s-node-1    Ready             23d   v1.18.1
k8s-node-2    Ready             23d   v1.18.1

Ceph 版本：
$ ceph version
ceph version 14.2.11 (f7fdb2f52131f54b891a2ec99d8205561242cdaf) nautilus (stable)

以下是详细部署过程：

01  新建Ceph Pool 
 
创建一个新的 ceph 存储池（pool） 给 Kubernetes 使用：
$ ceph osd pool create kubernetes 64 64
pool 'kubernetes' created

查看所有的 pool
$ ceph osd lspools

1 cephfs_data
2 cephfs_metadata
3 .rgw.root
4 default.rgw.control
5 default.rgw.meta
6 default.rgw.log
7 kubernetes

 
02  新建用户
 
为 Kubernetes 和 ceph-csi 单独创建一个新用户：
$ ceph auth get-or-create client.kubernetes mon 'profile rbd' osd 'profile rbd pool=kubernetes' mgr 'profile rbd pool=kubernetes'

[client.kubernetes]
    key = AQBnz11fclrxChAAf8TFw8ROzmr8ifftAHQbTw==
    
后面的配置需要用到这里的 key，如果忘了可以通过以下命令来获取：
$ ceph auth get client.kubernetes
exported keyring for client.kubernetes
[client.kubernetes]
 key = AQBnz11fclrxChAAf8TFw8ROzmr8ifftAHQbTw==
 caps mgr = "profile rbd pool=kubernetes"
 caps mon = "profile rbd"
 caps osd = "profile rbd pool=kubernetes"

 
03 部署ceph-csi
 
拉取 ceph-csi 的最新 release 分支：
$ git clone --depth 1 --branch v3.1.0 https://gitclone.com/github.com/ceph/ceph-csi
这里使用 gitclone 来加速拉取。

获取 ceph 集群的信息：
$ ceph mon dump

dumped monmap epoch 1
epoch 1
fsid 154c3d17-a9af-4f52-b83e-0fddd5db6e1b
last_changed 2020-09-12 16:16:53.774567
created 2020-09-12 16:16:53.774567
min_mon_release 14 (nautilus)
0: [v2:172.16.1.21:3300/0,v1:172.16.1.21:6789/0] mon.sealos01
1: [v2:172.16.1.22:3300/0,v1:172.16.1.22:6789/0] mon.sealos02
2: [v2:172.16.1.23:3300/0,v1:172.16.1.23:6789/0] mon.sealos03

这里需要用到两个信息：
fsid : 这个是 Ceph 的集群 ID。
监控节点信息。目前 ceph-csi 只支持 V1版本的协议，所以监控节点那里我们只能用 V1 的那个 IP 和端口号（例如, 172.16.1.21:6789 ）。

进入 ceph-csi 的 deploy/rbd/kubernetes 目录：
$ cd deploy/rbd/kubernetes

$ ls -l 
-rw-r--r-- 1 root root  100 Sep 14 04:49 csi-config-map.yaml
-rw-r--r-- 1 root root 1686 Sep 14 04:49 csi-nodeplugin-psp.yaml
-rw-r--r-- 1 root root  858 Sep 14 04:49 csi-nodeplugin-rbac.yaml
-rw-r--r-- 1 root root 1312 Sep 14 04:49 csi-provisioner-psp.yaml
-rw-r--r-- 1 root root 3105 Sep 14 04:49 csi-provisioner-rbac.yaml
-rw-r--r-- 1 root root 5497 Sep 14 04:49 csi-rbdplugin-provisioner.yaml
-rw-r--r-- 1 root root 5852 Sep 14 04:49 csi-rbdplugin.yaml

修改 Configmap, fsid为clusterID
将以上获取的信息写入 csi-config-map.yaml : 
---
apiVersion: v1
kind: ConfigMap
data:
  config.json: |-
    [
      {
        "clusterID": "154c3d17-a9af-4f52-b83e-0fddd5db6e1b",
        "monitors": [
          "172.16.1.21:6789",
          "172.15.1.22:6789",
          "172.16.1.23:6789"
        ]
      }
    ]
metadata:
  name: ceph-csi-config
  
创建一个新的 namespace 专门用来部署 ceph-csi：
$ kubectl create ns ceph-csi

将此 Configmap 存储到 Kubernetes 集群中：
$ kubectl -n ceph-csi apply -f csi-config-map.yaml
新建 Secret

使用创建的 kubernetes 用户 ID 和 cephx 密钥生成 Secret ：
cat <<EOF > csi-rbd-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: csi-rbd-secret
  namespace: ceph-csi
stringData:
  userID: kubernetes
  userKey: AQBnz11fclrxChAAf8TFw8ROzmr8ifftAHQbTw==
EOF

部署 Secret：
$ kubectl apply -f csi-rbd-secret.yaml

RBAC 授权

将所有配置清单中的 namespace: default 改成 namespace: ceph-csi ：
$ sed -i "s/namespace: default/namespace: ceph-csi/g" $(grep -rl "namespace: default" ./)
$ sed -i -e "/^kind: ServiceAccount/{N;N;a\  namespace: ceph-csi }" $(egrep -rl "^kind: ServiceAccount" ./)

创建必须的 ServiceAccount 和 RBAC ClusterRole/ClusterRoleBinding 资源对象：
$ kubectl create -f csi-provisioner-rbac.yaml
$ kubectl create -f csi-nodeplugin-rbac.yaml

创建 PodSecurityPolicy：
$ kubectl create -f csi-provisioner-psp.yaml
$ kubectl create -f csi-nodeplugin-psp.yaml

部署 CSI sidecar

将 csi-rbdplugin-provisioner.yaml 和 csi-rbdplugin.yaml 文件中的 kms 部分配置：
使用 # 注释掉
      #- name: ceph-csi-encryption-kms-config
      #  mountPath: /etc/ceph-csi-encryption-kms-config/

2个文件应该有多处需要 # 注释

部署csi-rbdplugin-provisioner：
$ kubectl -n ceph-csi create -f csi-rbdplugin-provisioner.yaml
这里面包含了 6 个 Sidecar 容器: 包括 external-provisioner、external-attacher、csi-resizer、 和 csi-rbdplugin 。
部署 RBD CSI driver

最后部署 RBD CSI Driver ：
$ kubectl -n ceph-csi create -f csi-rbdplugin.yaml
Pod 中包含两个容器：CSI node-driver-registrar 和CSI RBD Driver 。

创建 Storageclass

$ cat <<EOF > storageclass.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: csi-rbd-sc
provisioner: rbd.csi.ceph.com
parameters:
   clusterID: 154c3d17-a9af-4f52-b83e-0fddd5db6e1b
   pool: kubernetes
   imageFeatures: layering
   csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
   csi.storage.k8s.io/provisioner-secret-namespace: ceph-csi
   csi.storage.k8s.io/controller-expand-secret-name: csi-rbd-secret
   csi.storage.k8s.io/controller-expand-secret-namespace: ceph-csi
   csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
   csi.storage.k8s.io/node-stage-secret-namespace: ceph-csi
   csi.storage.k8s.io/fstype: ext4
reclaimPolicy: Delete
allowVolumeExpansion: true
mountOptions:
   - discard
EOF

这里的 clusterID  对应之前步骤中的 fsid 。
imageFeatures 用来确定创建的 image 特征，如果不指定，就会使用 RBD 内核中的特征列表，但 Linux 不一定支持所有特征，所以这里需要限制一下。

 
04 试用 ceph-csi
 
Kubernetes 通过 persistentVolume 子系统为用户和管理员提供了一组 API，将存储如何供应的细节从其如何被使用中抽象出来，其中 PV（PersistentVolume） 是实际的存储，pvc（PersistentVolumeClaim） 是用户对存储的请求。
下面通过官方仓库的示例来演示如何使用 ceph-csi。

先进入 ceph-csi 项目的 example/rbd 目录，然后直接创建 PVC：
$ kubectl apply -f pvc.yaml

查看 PVC 和申请成功的 PV：
$ kubectl get pvc
NAME      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
rbd-pvc   Bound    pvc-44b89f0e-4efd-4396-9316-10a04d289d7f   1Gi        RWO            csi-rbd-sc     8m21s

$ kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                STORAGECLASS   REASON   AGE
pvc-44b89f0e-4efd-4396-9316-10a04d289d7f   1Gi        RWO            Delete           Bound    default/rbd-pvc      csi-rbd-sc              8m18s

再创建示例 Pod：
$ kubectl apply -f pod.yaml

进入 Pod 里面测试读写数据：
$ kubectl exec -it csi-rbd-demo-pod bash
root@csi-rbd-demo-pod:/# cd /var/lib/www/
root@csi-rbd-demo-pod:/var/lib/www# ls -l
total 4
drwxrwxrwx 3 root root 4096 Sep 14 09:09 html
root@csi-rbd-demo-pod:/var/lib/www# echo "https://fuckcloudnative.io" > sealos.txt
root@csi-rbd-demo-pod:/var/lib/www# cat sealos.txt
https://fuckcloudnative.io

列出 kubernetes pool 中的 rbd images ：
$ rbd ls -p kubernetes
csi-vol-d9d011f9-f669-11ea-a3fa-ee21730897e6
查看该 image 的特征：
$ rbd info csi-vol-d9d011f9-f669-11ea-a3fa-ee21730897e6 -p kubernetes
rbd image 'csi-vol-d9d011f9-f669-11ea-a3fa-ee21730897e6':
 size 1 GiB in 256 objects
 order 22 (4 MiB objects)
 snapshot_count: 0
 id: 8da46585bb36
 block_name_prefix: rbd_data.8da46585bb36
 format: 2
 features: layering
 op_features:
 flags:
 create_timestamp: Mon Sep 14 09:08:27 2020
 access_timestamp: Mon Sep 14 09:08:27 2020
 modify_timestamp: Mon Sep 14 09:08:27 2020

可以看到对 image 的特征限制生效了，这里只有layering。
实际上这个 image 会被挂载到 node 中作为一个块设备，可以通过 rbd 命令查看映射信息：
$ rbd showmapped
id pool       namespace image                                        snap device
0  kubernetes           csi-vol-d9d011f9-f669-11ea-a3fa-ee21730897e6 -    /dev/rbd0

在 node 上查看挂载信息：
$ lsblk -l|grep rbd
rbd0   252:32   0  1G  0 disk /var/lib/kubelet/pods/15179e76-e06e-4c0e-91dc-e6ecf2119f4b/volumes/kubernetes.io~csi/pvc-44b89f0e-4efd-4396-9316-10a04d289d7f/mount

在 容器中查看挂载信息：
$ kubectl exec -it csi-rbd-demo-pod bash
root@csi-rbd-demo-pod:/# lsblk -l|grep rbd
rbd0   252:32   0     1G  0 disk /var/lib/www/html

一切正常！
